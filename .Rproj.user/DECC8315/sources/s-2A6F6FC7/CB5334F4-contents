---
title: "**Calculating Inter-Rater Reliability Coefficients / 1**"
subtitle: "*Chance-corrected Agreement Coefficients / CAC*"
author: "Author: Kilem L. Gwet, Ph.D. (gwet@agreestat.com)"
date:  "`r format(Sys.time(), '%d %B %Y')`"
#abstract:
output: html_document
runtime: shiny
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
library(irrCAC)
library(formattable)
```
### **Abstract**
<p style="font-family: times, serif; font-size:13pt; font-style:italic">
The primary goal of this interactive document is to provide researchers and students with a short and engaging introduction to the the 6 most widely-used chance-agreement agreement coefficients. A user-friendly inter-rater reliability calculator is embedded to this document and you are encouraged to experiment with it using your own numbers. You will see how different numbers affect the various agreement coefficients.
</p>

### **Introduction**

<font size="3"> 
Several chance-corrected agreement coefficients have been proposed in the inter-rater reliability literature.  Deciding which of these coefficients to report in your research can be challenging.  I have written extensively on this topic and have my preferences. The next section shows an online inter-rater reliability calculator that you can play with and see in realtime how changing the input values on the left impacts the coefficients of the right. 

My book ["Handbook of Inter-Rater Reliability"](https://www.amazon.com/Handbook-Inter-Rater-Reliability-Definitive-Measuring/dp/0970806280/) (Gwet, 2014) provides a comprehensive review of various techniques currently in use. To learn more, see [<strong>my inter-rater reliability blog</strong>](http://inter-rater-reliability.blogspot.com/) where you can find discussions on various topics on interest.
</font> 

### **The Calculator**

<font size="3"> 
This online calculator can perform a detailed analysis of the <r style="color:red"><strong>`r reactive(inputdat$q)`$\times$`r reactive(inputdat$q)`</strong></r> matrix on the left and produces 6 different agreement coefficients on the right side along with their associated precision measures. The problem addressed here is that of 2 raters that must classify each of the <r style="color:red"><strong>`r reactive(inputdat$n)`</strong></r> subjects into one of <r style="color:red"><strong>`r reactive(inputdat$q)`</strong></r> categories. The extent to which they agree is quantified by one of the agreement coefficients. <r style="color:blue">I invite you to change the number of categories using the combobox, and to modify the cell values to automatically get updated agreement coefficients on the right.!</r>
</font> 

```{r eruptions, echo=FALSE}

#--------------------> UI  --------------------------

sidebarLayout(
  sidebarPanel(width=5,
    fluidRow(
      column(width=4,helpText("# Categories")),
      column(width=3,selectInput("qCateg", label = NULL, choices = c(2:4),selected = "2",width="3cm")),
      column(width=1,checkboxInput(inputId="chklabels",label=NULL,value=TRUE,width=NULL)),
      column(width=2,actionButton("update",HTML("<strong>Update</strong>"),class = "btn action-button",
                                  style = "color: white;background-color: darkred"))

    ),
    fluidRow(
      conditionalPanel(
        condition="input.chklabels==true",
          uiOutput("custcateg2RC"),
          textInput(inputId="ti.clabels2RC",label=NULL,value="",placeholder = "Categ1,Categ2")
      ),
      uiOutput("Categories"),
      uiOutput("Ratings")
    )
  ), #End of sidebarPanel()
  mainPanel(width=7,
        uiOutput("titCoeff"), # print the title "INTER-RATER RELIABILITY COEFFICIENTS & ASSOCIATED PRECISION..."
        br(),
        tableOutput("gridOutUnwt")
  ) #End of mainPanel(  
)#sidebarLayout(


#----------------------> SERVER  ----------------------------


#- Compute unweighted coefficients
coefficients2C.f <- function(wtmat.p,xratings){
  coeffmat.p = kappa2.table(xratings,weights=wtmat.p)
  coeffmat.p = rbind(coeffmat.p,gwet.ac1.table(xratings,weights=wtmat.p))
  coeffmat.p = rbind(coeffmat.p,scott2.table(xratings,weights=wtmat.p))
  coeffmat.p = rbind(coeffmat.p,krippen2.table(xratings,weights=wtmat.p))
  coeffmat.p = rbind(coeffmat.p,bp2.table(xratings,weights=wtmat.p))
  coeffmat.p = rbind(coeffmat.p,pa2.table(xratings,weights=wtmat.p))
  colnames(coeffmat.p) <- c("method","coeff","stderr","conf.interv", "p.value")
  return(coeffmat.p)
}
read.ratings <- function(q){
  in.data <- matrix(sample(q,q^2,replace=TRUE),q)
  diag(in.data) <- sample(q:(15*q),q,replace=TRUE)
  for (k in 1:q){
    for (l in 1:q){
      in.data[k,l] <- as.numeric(input[[paste0('Cells2C',k,l)]])
      if (is.na(in.data[k,l])) in.data[k,l] <- 0
    }
  }
  return(in.data)
}
clabels <- reactive(clabels <- str_trim(unlist(strsplit(input$ti.clabels2RC,","))))
output$custcateg2RC <- renderUI(HTML("Customize Category Labels <i>(Separated by Comma)</i>"))
output$Categories <-renderUI( #display category labels in the first row
  fluidRow(
    column(1, helpText("")),
    lapply(1:input$qCateg, function(j){
      if (!is.na(clabels()[j])) column(width=1, helpText(substr(clabels()[j],1,6)),offset=1)
      else column(width=1, helpText(paste0('Categ', j)),offset=1)
    })
  )
)
nkl.mat <- reactive({
  q <- as.numeric(input$qCateg)
  if (!is.null(input[[paste0('Cells2C',q,q)]])) read.ratings(q)
  else{
    init.mat<-matrix(sample(q,q^2,replace=TRUE),q)
    diag(init.mat)<-sample(q:(15*q),q,replace=TRUE)
    init.mat
  } 
})
# display all ratings table cells and category labels of the first column
output$Ratings <- renderUI(
  lapply(1:input$qCateg, function(i) {
    fluidRow(
      if (!is.na(clabels()[i])) column(width=1, helpText(substr(clabels()[i],1,6)))
      else column(width=1, helpText(paste0('Categ', i))),
      lapply(1:input$qCateg, function(j) {
        column(width=1, textInput(paste0("Cells2C",i,j), label = NULL,value = isolate(nkl.mat()[i,j])),
               tags$style(type='text/css',paste0("#Cells2C",i,j," { width: 55px;text-align:center;}")),offset = 1)
      })
    )
  })
)
# Read ratings from the contingency table
coeff <- reactiveValues(kappa=0,ac1=0,pi=0,alpha=0,bp=0,pa=0)
inputdat <- reactiveValues(q=2,n=0)
diag <- reactiveValues(sum=0)
pe <- reactiveValues(kappa=0,ac1=0,pi=0,alpha=0,bp=0)
pa <- reactiveValues(alpha=0)

# observeEvent(input$update,{
reactive({
  input$update
  output$titCoeff <- renderUI(HTML("<font size=4><b>Inter-rater reliability coefficients & precision measures</b></font>"))
  q <- as.numeric(input$qCateg)
  inputdat$q <- q
  isolate({      
      DF.Out2RC <- nkl.mat()
      inputdat$n <- sum(DF.Out2RC)
      pikl.vec <- round((rowSums(DF.Out2RC)+colSums(DF.Out2RC))/(2*inputdat$n),3)
      pe$kappa <- round(sum(rowSums(DF.Out2RC)*colSums(DF.Out2RC)/(inputdat$n^2)),3)
      pe$ac1 <- round(sum(pikl.vec*(1-pikl.vec))/(q-1),3)
      pe$pi <- round(sum(pikl.vec^2),3)
      pe$alpha <- round(sum(pikl.vec^2),3)
      pe$bp <- round(1/q,3)
      
      DF.Unweighted2RC <- coefficients2C.f(identity.weights(1:isolate(input$qCateg)),DF.Out2RC)
      DF.Unweighted2RC$coeff <- format(DF.Unweighted2RC$coeff,digits=3) 
      DF.Unweighted2RC$stderr <- format(DF.Unweighted2RC$stderr,digits=2,nsmall=3)
      
      dsum <-paste(sapply(1:q,function(k) paste0("+",DF.Out2RC[k,k])),collapse = "")
      diag$sum <- substring(dsum,2)
      coeff$kappa <- DF.Unweighted2RC$coeff[1];coeff$ac1 <-DF.Unweighted2RC$coeff[2];
      coeff$pi <- DF.Unweighted2RC$coeff[3];coeff$alpha <-DF.Unweighted2RC$coeff[4];
      coeff$bp <- DF.Unweighted2RC$coeff[5];coeff$pa <-DF.Unweighted2RC$coeff[6]
      pa$alpha <- round((1-1/(2*inputdat$n))*as.numeric(unlist(coeff$pa)) + 1/(2*inputdat$n),3)
      colnames(DF.Unweighted2RC) <- c("Method","Coeff","StdErr","95% C.I","P-Value")
      output$gridOutUnwt = renderTable(DF.Unweighted2RC,bordered = TRUE,spacing = 'xs',striped = TRUE)
  })
  n <- reactive(inputdat$n)
})

```

<font size="3"> 
Cohen's Kappa coefficient is $\kappa=$ <r style="color:red"><strong>`r reactive(coeff$kappa)`</strong></r> and Gwet $AC_1$ coefficient is $AC_1=$ <r style="color:red"><strong>`r reactive(coeff$ac1)`</strong></r>. Scott's Pi coefficient is $\pi=$ <r style="color:red"><strong>`r reactive(coeff$pi)`</strong></r> and Krippendorff's alpha is given by, $\alpha=$ <r style="color:red"><strong>`r reactive(coeff$alpha)`</strong></r>. The unadjusted percent agreement is $p_a=$ <r style="color:red"><strong>`r reactive(coeff$pa)`</strong></r>. 

<r style="color:blue">Do you want to see how badly Cohen's Kappa, Krippendorff's alpha and Scott's Pi can perform?</r>
Consider 2 categories and an 2x2 input matrix where the 2 diagonal elements are 58 and 0, and the 2 off-diagonal elements are 4 and 1.  Then look at the outcome on the right side. You make your own opinion.
</font>

### **The Agreement Coefficients**

<font size="3">
Almost all chance-corrected agreement coefficients can be formulated as $\widehat{\kappa}=(p_a-p_e)/(1-p_e)$ where $p_a$ is the percent agreement and $p_e$ the percent chance agreement. Almost all agreement coefficients share a common percent agreement, which represents the relative number of subjects on the diagonal. With the current <r style="color:red"><strong>`r reactive(inputdat$q)` </strong></r> categories, the percent agreement is calculated as follows: <r style="color:red"><strong>$\pmb{p_a}=($`r reactive(diag$sum)`$)/$`r reactive(inputdat$n)`$=$`r reactive(coeff$pa)`</strong></r>. Only Krippendorff's alpha coefficient has a different percent agreement. This one is given by, $p_a^\prime = [1-1/(2n)]p_a+1/(2n)$= <r style="color:red"><strong> $[1-1/(2\times$`r reactive(inputdat$n)` $)]\times$`r reactive(coeff$pa)` $+1/(2\times$`r reactive(inputdat$n)` $)=$ `r reactive(pa$alpha)` </strong></r>. The percent agreement associated with Krippendorff's alpha is generally close to the regular percent agreement, especially when the number of subjects is reasonably large.

* The percent chance agreement associated with Cohen's kappa coefficient is given by $p_e(\kappa)=$ <r style="color:red"><strong>`r reactive(pe$kappa)`</strong></r>.  It is obtained by first multiplying the 2 sets of marginal proportions elementwise, before summing the resulting vector.

* The percent chance agreement assciated with Gwet's $AC_1$ coefficient is given by $p_e(AC_1)=$ <r style="color:red"><strong>`r reactive(pe$ac1)`</strong></r>. The algebra that leads to this estimate can be found in Gwet (2008).

* The percent chance agreement assciated with Scott's Pi and that associated with Krippendorff's alpha are always identical whenever each rater rates all subjects. Their common value is give by: $p_e(\alpha)=$ <r style="color:red"><strong>`r reactive(pe$alpha)`</strong></r>. More information regarding the calculation of Krippendorff's coefficient can be [<strong>here</strong>](http://www.agreestat.com/research_papers/onkrippendorffalpha_rev10052015.pdf). Gwet (2008) also provides additional information on Scott's Pi coefficient.

* The percent chance agreement assciated with Brennan-Prediger coefficient is given by $p_e(BP)=$ <r style="color:red"><strong>`r reactive(pe$bp)`</strong></r>, and is calculated by taking the inverse of the number of categories.

You must know that for Cohen's kappa, Scott's Pi and Krippendorff's alpha, the percent chance agreement is sometimes and inexplicably undefined.  In this case, the associated coefficients will be undefined as well. This will typically be the case when both raters classifify all subjects into one of the 2 diagonal cells. Try this scenario with the online inter-rater reliability calculator to convince yourself.

I personnally consider these 3 coefficients to be unreliable.  Consider for example a situation where 2 raters rate 57 subjects, 55 of whom are classified in category 1 by both raters, a single subject is classified into category 2 by both raters, and one subject is classified into category 1 by one rater and in category 2 by the other (you may enter these numbers in the calculator to see what happens). Cohen's kappa, Scott's Pi and Krippendorff's alpha coefficients respectively take the following values: 0.659, 0.658, and 0.661, which indicates a reasonnably high agreement. However, if you make a slight change to your input data so that the one subject that was assigned into category 2 by both raters is now assigned into category 2 by rater 1 and into category 1 by rater 2. Then suddenly all 3 agreement coefficients now produce negative values. Why such a dramatic effect to such a tiny change? I am yet to see a credible explanation to this very strange behavior.

</font>


### **References**

<font size="3">

* Cohen, J. (1960). "A coefficient of agreement for nominal scales." *Educational and Psychological Measurement*, 20, 37-46.

* Gwet, K. L. (2008). [Computing inter-rater reliability and its variance in the presence of high agreement.](http://www.agreestat.com/research_papers/bjmsp2008_interrater.pdf/) *British Journal of Mathematical and Statistical Psychology*, 61, 29-48.

* Gwet, K.L. (2014) *Handbook of Inter-Rater Reliability*, 4th Edition. Advanced Analytics, LLC.

</font>